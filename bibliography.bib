@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{devlin2019bert,
  title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
  author = "Devlin, Jacob  and
    Chang, Ming-Wei  and
    Lee, Kenton  and
    Toutanova, Kristina",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
  month = jun,
  year = "2019",
  address = "Minneapolis, Minnesota",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N19-1423",
  doi = "10.18653/v1/N19-1423",
  pages = "4171--4186",
  abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{zhu2023chatgpt,
  title={Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
  author={Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2303.06594},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{su2022magic,
  title={Language models can see: plugging visual controls in text generation},
  author={Su, Yixuan and Lan, Tian and Liu, Yahui and Liu, Fangyu and Yogatama, Dani and Wang, Yan and Kong, Lingpeng and Collier, Nigel},
  journal={arXiv preprint arXiv:2205.02655},
  year={2022}
}

@inproceedings{wang2022vidil,
  title={Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners},
  author={Zhenhailong Wang and Manling Li and Ruochen Xu and Luowei Zhou and Jie Lei and Xudong Lin and Shuohang Wang and Ziyi Yang and Chenguang Zhu and Derek Hoiem and Shih-Fu Chang and Mohit Bansal and Heng Ji},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=_LceCyuVcH}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zellers2021merlot,
  title={Merlot: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23634--23651},
  year={2021}
}

@inproceedings{zellers2022mreserve,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16375--16387},
  year={2022}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{driess2023palme,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{buch2022revisiting,
  author={Buch, Shyamal and Eyzaguirre, Cristóbal and Gaidon, Adrien and Wu, Jiajun and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Revisiting the “Video” in Video-Language Understanding}, 
  year={2022},
  volume={},
  number={},
  pages={2907-2917},
  doi={10.1109/CVPR52688.2022.00293}
}

@inproceedings{zeng2023socratic,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Maria Attarian and brian ichter and Krzysztof Marcin Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael S Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=G2Q2Mh3avow}
}

@misc{creswell2022faithful,
  title={Faithful Reasoning Using Large Language Models}, 
  author={Antonia Creswell and Murray Shanahan},
  year={2022},
  eprint={2208.14271},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}

@inproceedings{bagad2023testoftime,
  title={{T}est of {T}ime: {I}nstilling {V}ideo-{L}anguage {M}odels with a {S}ense of {T}ime},
  author={Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2023}
}

@article{wu2023audio,
  title={Audio-Text Models Do Not Yet Leverage Natural Language},
  author={Wu, Ho-Hsiang and Nieto, Oriol and Bello, Juan Pablo and Salamon, Justin},
  journal={arXiv preprint arXiv:2303.10667},
  year={2023}
}

@inproceedings{lei2020tvqaplus,
  title = "{TVQA}+: Spatio-Temporal Grounding for Video Question Answering",
  author = "Lei, Jie  and
    Yu, Licheng  and
    Berg, Tamara  and
    Bansal, Mohit",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.730",
  doi = "10.18653/v1/2020.acl-main.730",
  pages = "8211--8225",
  abstract = "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",
}

@inproceedings{momeni2023verbs,
  title={Verbs in Action: Improving verb understanding in video-language models},
  author={Momeni, Liliane and Caron, Mathilde and Nagrani, Arsha and Zisserman, Andrew and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15579--15591},
  year={2023}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}

@article{gan2022vision,
  title={Vision-language pre-training: Basics, recent advances, and future trends},
  author={Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume={14},
  number={3--4},
  pages={163--352},
  year={2022},
  publisher={Now Publishers, Inc.}
}

@inproceedings{yuksekgonul2023when,
  title={When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?},
  author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=KRLUvxh8uaX}
}

@inproceedings{lei2021clipbert,
  author    = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
  title     = {Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2021},
  pages     = {7331-7341}
}

@inproceedings{xu2021videoclip,
  title = "{V}ideo{CLIP}: Contrastive Pre-training for Zero-shot Video-Text Understanding",
  author = "Xu, Hu  and
    Ghosh, Gargi  and
    Huang, Po-Yao  and
    Okhonko, Dmytro  and
    Aghajanyan, Armen  and
    Metze, Florian  and
    Zettlemoyer, Luke  and
    Feichtenhofer, Christoph",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.544",
  doi = "10.18653/v1/2021.emnlp-main.544",
  pages = "6787--6800",
  abstract = "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.",
}

@inproceedings{xu2021vlm,
  title = "{VLM}: Task-agnostic Video-Language Model Pre-training for Video Understanding",
  author = "Xu, Hu  and
    Ghosh, Gargi  and
    Huang, Po-Yao  and
    Arora, Prahal  and
    Aminzadeh, Masoumeh  and
    Feichtenhofer, Christoph  and
    Metze, Florian  and
    Zettlemoyer, Luke",
  booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
  month = aug,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.findings-acl.370",
  doi = "10.18653/v1/2021.findings-acl.370",
  pages = "4227--4239",
}

@inproceedings{lin2022evl,
  author = {Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  title = {Frozen CLIP Models Are Efficient Video Learners},
  year = {2022},
  isbn = {978-3-031-19832-8},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-031-19833-5_23},
  doi = {10.1007/978-3-031-19833-5_23},
  abstract = {Video recognition has been dominated by the end-to-end learning paradigm – first initializing a video recognition model with weights of a pretrained image model and then conducting end-to-end training on videos. This enables the video network to benefit from the pretrained image model. However, this requires substantial computation and memory resources for finetuning on videos and the alternative of directly using pretrained image features without finetuning the image backbone leads to subpar results. Fortunately, recent advances in Contrastive Vision-Language Pre-training (CLIP) pave the way for a new route for visual recognition tasks. Pretrained on large open-vocabulary image–text pair data, these models learn powerful visual representations with rich semantics. In this paper, we present Efficient Video Learning (EVL) – an efficient framework for directly training high-quality video recognition models with frozen CLIP features. Specifically, we employ a lightweight Transformer decoder and learn a query token to dynamically collect frame-level spatial features from the CLIP image encoder. Furthermore, we adopt a local temporal module in each decoder layer to discover temporal clues from adjacent frames and their attention maps. We show that despite being efficient to train with a frozen backbone, our models learn high quality video representations on a variety of video recognition datasets. Code is available at .},
  booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV},
  pages = {388–404},
  numpages = {17},
  keywords = {Spatiotemporal fusion, Efficient learning, Vision-language model, Video recognition},
  location = {Tel Aviv, Israel}
}

@inproceedings{wu2021star,
  author={Wu, Bo and Yu, Shoubin and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  title = {{STAR}: A Benchmark for Situated Reasoning in Real-World Videos},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@inproceedings{dosovitskiy2021vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{krizhevsky2012alexnet,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {1097–1105},
  numpages = {9},
  location = {Lake Tahoe, Nevada},
  series = {NIPS'12}
}

@article{allen1983interval,
  author = {Allen, James F.},
  title = {Maintaining Knowledge about Temporal Intervals},
  year = {1983},
  issue_date = {Nov. 1983},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {26},
  number = {11},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/182.358434},
  doi = {10.1145/182.358434},
  journal = {Commun. ACM},
  month = nov,
  pages = {832–843},
  numpages = {12},
  keywords = {interval reasoning, temporal interval, interval representation}
}

@article{bruce1972temporalqa,
  title = {A model for temporal references and its application in a question answering program},
  journal = {Artificial Intelligence},
  volume = {3},
  pages = {1-25},
  year = {1972},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(72)90040-9},
  url = {https://www.sciencedirect.com/science/article/pii/0004370272900409},
  author = {Bertram {C. Bruce}},
}

@inproceedings{zhou2021tracie,
  title = "Temporal Reasoning on Implicit Events from Distant Supervision",
  author = "Zhou, Ben  and
    Richardson, Kyle  and
    Ning, Qiang  and
    Khot, Tushar  and
    Sabharwal, Ashish  and
    Roth, Dan",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.107",
  doi = "10.18653/v1/2021.naacl-main.107",
  pages = "1361--1371",
}

@INPROCEEDINGS{he2016resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@article{oord2019infonce,
  title={Representation Learning with Contrastive Predictive Coding}, 
  author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  year={2019},
  eprint={1807.03748},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@inproceedings{bender2020climbing,
  title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
  author = "Bender, Emily M.  and
    Koller, Alexander",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.463",
  doi = "10.18653/v1/2020.acl-main.463",
  pages = "5185--5198",
}

@misc{li2019visualbert,
  title={VisualBERT: A Simple and Performant Baseline for Vision and Language}, 
  author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  year={2019},
  eprint={1908.03557},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inbook{lu2019vilbert,
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  year = {2019},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  articleno = {2},
  numpages = {11}
}

@article{ouyang2022instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@InProceedings{xiao2021nextqa,
    author    = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
    title     = {NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {9777-9786}
}

@inproceedings{brown2020gpt3,
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume = {33},
  year = {2020}
}

@INPROCEEDINGS{zellers2019vcr,
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={From Recognition to Cognition: Visual Commonsense Reasoning},
  year={2019},
  volume={},
  number={},
  pages={6713-6724},
  doi={10.1109/CVPR.2019.00688}
}

@article{sigurdsson2016charades,
    author = {Gunnar A. Sigurdsson and G{\"u}l Varol and Xiaolong Wang and Ivan Laptev and Ali Farhadi and Abhinav Gupta},
    title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},
    journal = {ArXiv e-prints},
    eprint = {1604.01753}, 
    year = {2016},
    url = {http://arxiv.org/abs/1604.01753},
}

@INPROCEEDINGS{sevilla-lara2021temporal,
  author={Sevilla-Lara, Laura and Zha, Shengxin and Yan, Zhicheng and Goswami, Vedanuj and Feiszli, Matt and Torresani, Lorenzo},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title={Only Time Can Tell: Discovering Temporal Data for Temporal Modeling},
  year={2021},
  volume={},
  number={},
  pages={535-544},
  doi={10.1109/WACV48630.2021.00058}
}

@article{bengio2003nlm,
  author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
  title = {A Neural Probabilistic Language Model},
  year = {2003},
  issue_date = {3/1/2003},
  publisher = {JMLR.org},
  volume = {3},
  number = {null},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  month = {mar},
  pages = {1137–1155},
  numpages = {19}
}

@INPROCEEDINGS{hudson2019gqa,
  author={Hudson, Drew A. and Manning, Christopher D.},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering}, 
  year={2019},
  volume={},
  number={},
  pages={6693-6702},
  doi={10.1109/CVPR.2019.00686}
}



@InProceedings{antol2015vqa,
  author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
  title = {{VQA}: {V}isual {Q}uestion {A}nswering},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year = {2015},
}

@inproceedings{agrawal2016analyzing,
  title = "Analyzing the Behavior of Visual Question Answering Models",
  author = "Agrawal, Aishwarya  and
    Batra, Dhruv  and
    Parikh, Devi",
  booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2016",
  address = "Austin, Texas",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D16-1203",
  doi = "10.18653/v1/D16-1203",
  pages = "1955--1960",
}

@inproceedings{zhang2016yin,
  title={Yin and yang: Balancing and answering binary visual questions},
  author={Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5014--5022},
  year={2016}
}

@inproceedings{xu2016msr-vtt,
  author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
  year = {2016},
  month = {June},
  booktitle = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  url = {https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/},
}

@ARTICLE{hochreiter1997lstm,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  doi={10.1162/neco.1997.9.8.1735}
}

@InProceedings{yang2016san,
  author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  title = {Stacked Attention Networks for Image Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016}
} 

@misc{liu2020roberta,
  title={Ro{BERT}a: A Robustly Optimized {BERT} Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2020},
  url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{deng2009imagenet,
  AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  BOOKTITLE = {CVPR09},
  YEAR = {2009},
  BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@article{lecun1989lenet,
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  title = "{Backpropagation Applied to Handwritten Zip Code Recognition}",
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541-551},
  year = {1989},
  month = {12},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  url = {https://doi.org/10.1162/neco.1989.1.4.541},
  eprint = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
}

@inproceedings{carion2020detr,
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  title = {End-to-End Object Detection with Transformers},
  year = {2020},
  isbn = {978-3-030-58451-1},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-030-58452-8_13},
  doi = {10.1007/978-3-030-58452-8_13},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
  pages = {213–229},
  numpages = {17},
  location = {Glasgow, United Kingdom}
}

@misc{gong2021ast,
  title={{AST}: Audio Spectrogram Transformer}, 
  author={Yuan Gong and Yu-An Chung and James Glass},
  year={2021},
  eprint={2104.01778},
  archivePrefix={arXiv},
  primaryClass={cs.SD}
}

@misc{carreira2018kinetics600,
  title={A Short Note about Kinetics-600}, 
  author={Joao Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},
  year={2018},
  eprint={1808.01340},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@article{pustejovsky2003timeml,
  title={TimeML: Robust specification of event and temporal expressions in text.},
  author={Pustejovsky, James and Ingria, Robert and Setzer, Andrea and Katz, Graham},
  year={2003},
  journal={IWCS-5, Fifth International Workshop on Computational Semantics.}
}

@INPROCEEDINGS{huang2018videotemporal,
  author={Huang, De-An and Ramanathan, Vignesh and Mahajan, Dhruv and Torresani, Lorenzo and Paluri, Manohar and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets}, 
  year={2018},
  volume={},
  number={},
  pages={7366-7375},
  doi={10.1109/CVPR.2018.00769}
}

@inproceedings{wei2022cot,
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages = {24824--24837},
  publisher = {Curran Associates, Inc.},
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf},
  volume = {35},
  year = {2022}
}

@inproceedings{kojima2022step,
  author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages = {22199--22213},
  publisher = {Curran Associates, Inc.},
  title = {Large Language Models are Zero-Shot Reasoners},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
  volume = {35},
  year = {2022}
}

@article{soomro2012ucf101,
  title={UCF101: A dataset of 101 human actions classes from videos in the wild},
  author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  journal={CRCV-TR-12-01},
  year={2012}
}

@InProceedings{bain2021frozen,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@article{luo2022clip4clip,
  title = {CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning},
  journal = {Neurocomputing},
  volume = {508},
  pages = {293-304},
  year = {2022},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2022.07.028},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231222008876},
  author = {Huaishao Luo and Lei Ji and Ming Zhong and Yang Chen and Wen Lei and Nan Duan and Tianrui Li},
  keywords = {Video retrieval, Video captioning, CLIP},
  abstract = {Video clip retrieval and captioning tasks play an essential role in multimodal research and are the fundamental research problem for multimodal understanding and generation. The CLIP (Contrastive Language-Image Pre-training) model has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the image-text pretrained CLIP model to video-text tasks in an end-to-end manner. Furthermore, we conduct several empirical studies including 1) Whether image feature is enough for video-text retrieval and captioning? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text datasets, including MSR-VTT, MSVD, LSMDC, and DiDeMo for multimodal understanding and generation tasks.}
}

@InProceedings{portilloquintero2021clipvidret,
  author="Portillo-Quintero, Jes{\'u}s Andr{\'e}s
  and Ortiz-Bayliss, Jos{\'e} Carlos
  and Terashima-Mar{\'i}n, Hugo",
  editor="Roman-Rangel, Edgar
  and Kuri-Morales, {\'A}ngel Fernando
  and Mart{\'i}nez-Trinidad, Jos{\'e} Francisco
  and Carrasco-Ochoa, Jes{\'u}s Ariel
  and Olvera-L{\'o}pez, Jos{\'e} Arturo",
  title="A Straightforward Framework for Video Retrieval Using CLIP",
  booktitle="Pattern Recognition",
  year="2021",
  publisher="Springer International Publishing",
  address="Cham",
  pages="3--12",
  abstract="Video Retrieval is a challenging task where the task aims at matching a text query to a video or vice versa. Most of the existing approaches for addressing such a problem rely on annotations made by the users. Although simple, this approach is not always feasible in practice. In this work, we explore the application of the language-image model, CLIP, to obtain video representations without the need for said annotations. This model was explicitly trained to learn a common space where images and text can be compared. Using various techniques described in this document, we extended its application to videos, obtaining state-of-the-art results on the MSR-VTT and MSVD benchmarks.",
  isbn="978-3-030-77004-4"
}

@inproceedings{bahdanau2015attention,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
}

@inproceedings{vashishtha2020temporal,
  title = "Temporal Reasoning in Natural Language Inference",
  author = "Vashishtha, Siddharth  and
    Poliak, Adam  and
    Lal, Yash Kumar  and
    Van Durme, Benjamin  and
    White, Aaron Steven",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.findings-emnlp.363",
  doi = "10.18653/v1/2020.findings-emnlp.363",
  pages = "4070--4078",
  abstract = "We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event duration{---}how long an event lasts{---}and event ordering{---}how events are temporally arranged{---}into more than one million NLI examples. We use these datasets to investigate how well neural models trained on a popular NLI corpus capture these forms of temporal reasoning.",
}

@article{elman2019event,
  title={A model of event knowledge.},
  author={Elman, Jeffrey L and McRae, Ken},
  journal={Psychological Review},
  volume={126},
  number={2},
  pages={252},
  year={2019},
  publisher={American Psychological Association}
}

@INPROCEEDINGS{grauman2022ego4d,
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Fuegen, Christian and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
  title     = {Ego4D: Around the {W}orld in 3,000 {H}ours of {E}gocentric {V}ideo},
  booktitle   = {IEEE/CVF Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022}
}

@article{moens1988temporal,
  title = "Temporal Ontology and Temporal Reference",
  author = "Moens, Marc  and
    Steedman, Mark",
  journal = "Computational Linguistics",
  volume = "14",
  number = "2",
  year = "1988",
  url = "https://aclanthology.org/J88-2003",
  pages = "15--28",
}

@article{marcus1993penntreebank,
  title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
  author = "Marcus, Mitchell P.  and
    Santorini, Beatrice  and
    Marcinkiewicz, Mary Ann",
  journal = "Computational Linguistics",
  volume = "19",
  number = "2",
  year = "1993",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/J93-2004",
  pages = "313--330",
}

@InProceedings{miech2019howto100m,
  author = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  title = {HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2019}
} 

@inproceedings{shang2019vidor,
  title={Annotating Objects and Relations in User-Generated Videos},
  author={Shang, Xindi and Di, Donglin and Xiao, Junbin and Cao, Yu and Yang, Xun and Chua, Tat-Seng},
  booktitle={Proceedings of the 2019 on International Conference on Multimedia Retrieval},
  pages={279--287},
  year={2019},
  organization={ACM}
}


@article{thomee2016yfcc100m,
  title={YFCC100M: The New Data in Multimedia Research},
  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={64--73},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@InProceedings{johnson2017clevr,
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},
  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {July},
  year = {2017}
} 

@INPROCEEDINGS{goyal2017vqa,
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering}, 
  year={2017},
  volume={},
  number={},
  pages={6325-6334},
  doi={10.1109/CVPR.2017.670}
}

@inproceedings{lei2023revealing,
  title = "Revealing Single Frame Bias for Video-and-Language Learning",
  author = "Lei, Jie  and
    Berg, Tamara  and
    Bansal, Mohit",
  editor = "Rogers, Anna  and
    Boyd-Graber, Jordan  and
    Okazaki, Naoaki",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.29",
  doi = "10.18653/v1/2023.acl-long.29",
  pages = "487--507",
  abstract = "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong {``}static appearance bias{''} in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at \url{https://github.com/jayleicn/singularity}.",
}

@InProceedings{zhai2022scalingvit,
  author    = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  title     = {Scaling Vision Transformers},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2022},
  pages     = {12104-12113}
}

@article{yu2022coca,
  title	= {CoCa: Contrastive Captioners are Image-Text Foundation Models},
  author	= {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
  year	= {2022},
  URL	= {https://arxiv.org/abs/2205.01917},
  journal	= {Transactions on Machine Learning Research},
  volume	= {Aug 2022}
}

@inproceedings{hoffmann2022chinchilla,
  title={An empirical analysis of compute-optimal large language model training},
  author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=iBBcRUlOAPR}
}

@InProceedings{cheng2023vindlu,
  author    = {Cheng, Feng and Wang, Xizi and Lei, Jie and Crandall, David and Bansal, Mohit and Bertasius, Gedas},
  title     = {VindLU: A Recipe for Effective Video-and-Language Pretraining},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2023},
  pages     = {10739-10750}
}

@inproceedings{hendricks2018tempo,
  title = "Localizing Moments in Video with Temporal Language",
  author = "Hendricks, Lisa Anne  and
    Wang, Oliver  and
    Shechtman, Eli  and
    Sivic, Josef  and
    Darrell, Trevor  and
    Russell, Bryan",
  editor = "Riloff, Ellen  and
    Chiang, David  and
    Hockenmaier, Julia  and
    Tsujii, Jun{'}ichi",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  month = oct # "-" # nov,
  year = "2018",
  address = "Brussels, Belgium",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D18-1168",
  doi = "10.18653/v1/D18-1168",
  pages = "1380--1390",
  abstract = "Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",
}

@inproceedings{xu2017video,
  title={Video Question Answering via Gradually Refined Attention over Appearance and Motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={ACM Multimedia},
  year={2017}
}
