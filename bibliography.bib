@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{devlin2019bert,
  title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
  author = "Devlin, Jacob  and
    Chang, Ming-Wei  and
    Lee, Kenton  and
    Toutanova, Kristina",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
  month = jun,
  year = "2019",
  address = "Minneapolis, Minnesota",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/N19-1423",
  doi = "10.18653/v1/N19-1423",
  pages = "4171--4186",
  abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{zhu2023chatgpt,
  title={Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
  author={Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2303.06594},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{su2022magic,
  title={Language models can see: plugging visual controls in text generation},
  author={Su, Yixuan and Lan, Tian and Liu, Yahui and Liu, Fangyu and Yogatama, Dani and Wang, Yan and Kong, Lingpeng and Collier, Nigel},
  journal={arXiv preprint arXiv:2205.02655},
  year={2022}
}

@inproceedings{wang2022vidil,
  title={Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners},
  author={Zhenhailong Wang and Manling Li and Ruochen Xu and Luowei Zhou and Jie Lei and Xudong Lin and Shuohang Wang and Ziyi Yang and Chenguang Zhu and Derek Hoiem and Shih-Fu Chang and Mohit Bansal and Heng Ji},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=_LceCyuVcH}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zellers2021merlot,
  title={Merlot: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23634--23651},
  year={2021}
}

@inproceedings{zellers2022mreserve,
  title={Merlot reserve: Neural script knowledge through vision and language and sound},
  author={Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16375--16387},
  year={2022}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{driess2023palme,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{buch2022revisiting,
  author={Buch, Shyamal and Eyzaguirre, Cristóbal and Gaidon, Adrien and Wu, Jiajun and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Revisiting the “Video” in Video-Language Understanding}, 
  year={2022},
  volume={},
  number={},
  pages={2907-2917},
  doi={10.1109/CVPR52688.2022.00293}
}

@inproceedings{zeng2023socratic,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Maria Attarian and brian ichter and Krzysztof Marcin Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael S Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=G2Q2Mh3avow}
}

@article{shanahan2022talking,
  title={Talking About Large Language Models},
  author={Shanahan, Murray},
  journal={arXiv preprint arXiv:2212.03551},
  year={2022}
}

@inproceedings{bagad2023testoftime,
  title={{T}est of {T}ime: {I}nstilling {V}ideo-{L}anguage {M}odels with a {S}ense of {T}ime},
  author={Bagad, Piyush and Tapaswi, Makarand and Snoek, Cees G. M.},
  booktitle={CVPR},
  year={2023}
}

@article{wu2023audio,
  title={Audio-Text Models Do Not Yet Leverage Natural Language},
  author={Wu, Ho-Hsiang and Nieto, Oriol and Bello, Juan Pablo and Salamon, Justin},
  journal={arXiv preprint arXiv:2303.10667},
  year={2023}
}

@inproceedings{lei2020tvqaplus,
  title = "{TVQA}+: Spatio-Temporal Grounding for Video Question Answering",
  author = "Lei, Jie  and
    Yu, Licheng  and
    Berg, Tamara  and
    Bansal, Mohit",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.730",
  doi = "10.18653/v1/2020.acl-main.730",
  pages = "8211--8225",
  abstract = "We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.",
}

@article{momeni2023verbs,
  title={Verbs in Action: Improving verb understanding in video-language models},
  author={Momeni, Liliane and Caron, Mathilde and Nagrani, Arsha and Zisserman, Andrew and Schmid, Cordelia},
  journal={arXiv preprint arXiv:2304.06708},
  year={2023}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7464--7473},
  year={2019}
}

@article{gan2022vision,
  title={Vision-language pre-training: Basics, recent advances, and future trends},
  author={Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume={14},
  number={3--4},
  pages={163--352},
  year={2022},
  publisher={Now Publishers, Inc.}
}

@inproceedings{yuksekgonul2023when,
  title={When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?},
  author={Mert Yuksekgonul and Federico Bianchi and Pratyusha Kalluri and Dan Jurafsky and James Zou},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=KRLUvxh8uaX}
}

@inproceedings{lei2021clipbert,
  author    = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L. and Bansal, Mohit and Liu, Jingjing},
  title     = {Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2021},
  pages     = {7331-7341}
}

@inproceedings{xu2021videoclip,
  title = "{V}ideo{CLIP}: Contrastive Pre-training for Zero-shot Video-Text Understanding",
  author = "Xu, Hu  and
    Ghosh, Gargi  and
    Huang, Po-Yao  and
    Okhonko, Dmytro  and
    Aghajanyan, Armen  and
    Metze, Florian  and
    Zettlemoyer, Luke  and
    Feichtenhofer, Christoph",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.544",
  doi = "10.18653/v1/2021.emnlp-main.544",
  pages = "6787--6800",
  abstract = "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.",
}

@inproceedings{xu2021vlm,
  title = "{VLM}: Task-agnostic Video-Language Model Pre-training for Video Understanding",
  author = "Xu, Hu  and
    Ghosh, Gargi  and
    Huang, Po-Yao  and
    Arora, Prahal  and
    Aminzadeh, Masoumeh  and
    Feichtenhofer, Christoph  and
    Metze, Florian  and
    Zettlemoyer, Luke",
  booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
  month = aug,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.findings-acl.370",
  doi = "10.18653/v1/2021.findings-acl.370",
  pages = "4227--4239",
}

@inproceedings{lin2022evl,
  author = {Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  title = {Frozen CLIP Models Are Efficient Video Learners},
  year = {2022},
  isbn = {978-3-031-19832-8},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-031-19833-5_23},
  doi = {10.1007/978-3-031-19833-5_23},
  abstract = {Video recognition has been dominated by the end-to-end learning paradigm – first initializing a video recognition model with weights of a pretrained image model and then conducting end-to-end training on videos. This enables the video network to benefit from the pretrained image model. However, this requires substantial computation and memory resources for finetuning on videos and the alternative of directly using pretrained image features without finetuning the image backbone leads to subpar results. Fortunately, recent advances in Contrastive Vision-Language Pre-training (CLIP) pave the way for a new route for visual recognition tasks. Pretrained on large open-vocabulary image–text pair data, these models learn powerful visual representations with rich semantics. In this paper, we present Efficient Video Learning (EVL) – an efficient framework for directly training high-quality video recognition models with frozen CLIP features. Specifically, we employ a lightweight Transformer decoder and learn a query token to dynamically collect frame-level spatial features from the CLIP image encoder. Furthermore, we adopt a local temporal module in each decoder layer to discover temporal clues from adjacent frames and their attention maps. We show that despite being efficient to train with a frozen backbone, our models learn high quality video representations on a variety of video recognition datasets. Code is available at .},
  booktitle = {Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV},
  pages = {388–404},
  numpages = {17},
  keywords = {Spatiotemporal fusion, Efficient learning, Vision-language model, Video recognition},
  location = {Tel Aviv, Israel}
}

@inproceedings{wu2021star,
  author={Wu, Bo and Yu, Shoubin and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  title = {{STAR}: A Benchmark for Situated Reasoning in Real-World Videos},
  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2021}
}

@inproceedings{dosovitskiy2021vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{krizhevsky2012alexnet,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2012},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  pages = {1097–1105},
  numpages = {9},
  location = {Lake Tahoe, Nevada},
  series = {NIPS'12}
}

@article{allen1983interval,
  author = {Allen, James F.},
  title = {Maintaining Knowledge about Temporal Intervals},
  year = {1983},
  issue_date = {Nov. 1983},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {26},
  number = {11},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/182.358434},
  doi = {10.1145/182.358434},
  journal = {Commun. ACM},
  month = nov,
  pages = {832–843},
  numpages = {12},
  keywords = {interval reasoning, temporal interval, interval representation}
}

@article{bruce1972temporalqa,
  title = {A model for temporal references and its application in a question answering program},
  journal = {Artificial Intelligence},
  volume = {3},
  pages = {1-25},
  year = {1972},
  issn = {0004-3702},
  doi = {https://doi.org/10.1016/0004-3702(72)90040-9},
  url = {https://www.sciencedirect.com/science/article/pii/0004370272900409},
  author = {Bertram {C. Bruce}},
}

@inproceedings{zhou2021tracie,
  title = "Temporal Reasoning on Implicit Events from Distant Supervision",
  author = "Zhou, Ben  and
    Richardson, Kyle  and
    Ning, Qiang  and
    Khot, Tushar  and
    Sabharwal, Ashish  and
    Roth, Dan",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
  month = jun,
  year = "2021",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.naacl-main.107",
  doi = "10.18653/v1/2021.naacl-main.107",
  pages = "1361--1371",
}

@INPROCEEDINGS{he2016resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@misc{oord2019infonce,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{bender2020climbing,
  title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
  author = "Bender, Emily M.  and
    Koller, Alexander",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
  month = jul,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.acl-main.463",
  doi = "10.18653/v1/2020.acl-main.463",
  pages = "5185--5198",
}

@misc{li2019visualbert,
  title={VisualBERT: A Simple and Performant Baseline for Vision and Language}, 
  author={Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  year={2019},
  eprint={1908.03557},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}

@inbook{lu2019vilbert,
  author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  year = {2019},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  articleno = {2},
  numpages = {11}
}

@article{ouyang2022instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@InProceedings{xiao2021nextqa,
    author    = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
    title     = {NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = June,
    year      = {2021},
    pages     = {9777-9786}
}

@misc{brown2020gpt3,
  title={Language Models are Few-Shot Learners}, 
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@INPROCEEDINGS{zellers2019vcr,
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title={From Recognition to Cognition: Visual Commonsense Reasoning},
  year={2019},
  volume={},
  number={},
  pages={6713-6724},
  doi={10.1109/CVPR.2019.00688}
}

@article{sigurdsson2016charades,
    author = {Gunnar A. Sigurdsson and G{\"u}l Varol and Xiaolong Wang and Ivan Laptev and Ali Farhadi and Abhinav Gupta},
    title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},
    journal = {ArXiv e-prints},
    eprint = {1604.01753}, 
    year = {2016},
    url = {http://arxiv.org/abs/1604.01753},
}

@INPROCEEDINGS{sevilla-lara2021temporal,
  author={Sevilla-Lara, Laura and Zha, Shengxin and Yan, Zhicheng and Goswami, Vedanuj and Feiszli, Matt and Torresani, Lorenzo},
  booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title={Only Time Can Tell: Discovering Temporal Data for Temporal Modeling},
  year={2021},
  volume={},
  number={},
  pages={535-544},
  doi={10.1109/WACV48630.2021.00058}
}

@article{bengio2003nlm,
  author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
  title = {A Neural Probabilistic Language Model},
  year = {2003},
  issue_date = {3/1/2003},
  publisher = {JMLR.org},
  volume = {3},
  number = {null},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  month = {mar},
  pages = {1137–1155},
  numpages = {19}
}

@INPROCEEDINGS{hudson2019gqa,
  author={Hudson, Drew A. and Manning, Christopher D.},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering}, 
  year={2019},
  volume={},
  number={},
  pages={6693-6702},
  doi={10.1109/CVPR.2019.00686}
}



@InProceedings{antol2015vqa,
  author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
  title = {{VQA}: {V}isual {Q}uestion {A}nswering},
  booktitle = {International Conference on Computer Vision (ICCV)},
  year = {2015},
}

@inproceedings{agrawal2016analyzing,
  title = "Analyzing the Behavior of Visual Question Answering Models",
  author = "Agrawal, Aishwarya  and
    Batra, Dhruv  and
    Parikh, Devi",
  booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2016",
  address = "Austin, Texas",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D16-1203",
  doi = "10.18653/v1/D16-1203",
  pages = "1955--1960",
}

@inproceedings{zhang2016yin,
  title={Yin and yang: Balancing and answering binary visual questions},
  author={Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5014--5022},
  year={2016}
}

@inproceedings{xu2016msr-vtt,
  author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  title = {MSR-VTT: A Large Video Description Dataset for Bridging Video and Language},
  year = {2016},
  month = {June},
  publisher = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
  url = {https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/},
}

@ARTICLE{hochreiter1997lstm,
  author={Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal={Neural Computation}, 
  title={Long Short-Term Memory}, 
  year={1997},
  volume={9},
  number={8},
  pages={1735-1780},
  doi={10.1162/neco.1997.9.8.1735}
}

@InProceedings{yang2016san,
  author = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  title = {Stacked Attention Networks for Image Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016}
} 

@misc{liu2020roberta,
  title={Ro{\{}BERT{\}}a: A Robustly Optimized {\{}BERT{\}} Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2020},
  url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{deng2009imagenet,
  AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  BOOKTITLE = {CVPR09},
  YEAR = {2009},
  BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"
}

@article{lecun1989lenet,
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  title = "{Backpropagation Applied to Handwritten Zip Code Recognition}",
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541-551},
  year = {1989},
  month = {12},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  url = {https://doi.org/10.1162/neco.1989.1.4.541},
  eprint = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
}

@inproceedings{carion2020detr,
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  title = {End-to-End Object Detection with Transformers},
  year = {2020},
  isbn = {978-3-030-58451-1},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-030-58452-8_13},
  doi = {10.1007/978-3-030-58452-8_13},
  booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I},
  pages = {213–229},
  numpages = {17},
  location = {Glasgow, United Kingdom}
}

@misc{gong2021ast,
  title={AST: Audio Spectrogram Transformer}, 
  author={Yuan Gong and Yu-An Chung and James Glass},
  year={2021},
  eprint={2104.01778},
  archivePrefix={arXiv},
  primaryClass={cs.SD}
}

@misc{carreira2018kinetics600,
  title={A Short Note about Kinetics-600}, 
  author={Joao Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},
  year={2018},
  eprint={1808.01340},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
