%! TeX root = ../charles/en/thesis.tex

A bit on how vision and language models perform well on many benchmarks. Then
talk about videos. How vision and language models have been extended to video
tasks via extra training on paired video-caption datasets, frozen LMs.

Introduce the problem. Main question: do they learn temporal reasoning? Are
models able to learn the difference between action X happening before action Y,
versus action X happening after action Y.

What do we do? - Discover that the answer is no, at least for Merlot Reserve.
How? Masking temporal indicator in STAR, no idea if it is before/after.
\XXX{Ondrej: A question for you or Raffa: Discovering that the answer is no is actually fortunate, the easier option. What is unclear is how would we convince ourselves that the answer is yes. What methodology would we use?}

Perhaps this should be explored for some other models as well (e.g. ClipBERT
\cite{lei2021clipbert}, Flamingo \cite{alayrac2022flamingo}, Frozen CLIP
\cite{lin2022evl}, VidIL \cite{wang2022vidil}, Socratic Models
\cite{zeng2023socratic}, VideoCLIP \cite{xu2021videoclip})
\XXX{Ondrej: yes! But primarily focus on the promised Merlot Reserve. If you can easily (one push of a button) do more, do as many as you can.}

So, mask temporal indicators in scripts, add negatives for cues.
\XXX{Ondrej: without knowing the details, I can't imagine any such cues yet. Maybe provide already an example in the introduction.}
E.g. before -\textgreater [after, at the same time, while]
Or possibly masking actions, how to generate negatives for this isn't clear.
Would need multiple masks potentially, quite complicated.

Train with either YT-Temporal-1B or Charades. Does it improve performance on
answering questions that require temporal information? If Charades, probably
need to do other datasets as well, e.g. NextQA, Epic Kitchens.

Hopefully the answer is that it performs better.
\XXX{For ``better'' you need a continuous measure (which you will certainly and easily have), and an improvement in this measure. Yet my high-level methodological question remains: What score in this measure would the model need in order to say ``the model does learn temporal reasoning''.}

The rest of this thesis is organised as follows:

\begin{itemize}
	\item \Cref{chap:bg} goes into the background of language models,
		image recognition models, and vision and language models which combine
		the two modalities. We discuss one popular method, Contrastive
		Language-Image Pretraining (CLIP), and one key downstream task, visual
		question answering. We finish the chapter with a broad overview of video
		language models.
	\item \Cref{chap:rel} explores related work on video language models,
		with a particular focus on work that explores the impact of contrastive
		pretraining. We also look at other approaches for instilling temporal
		reasoning in these models.
	\item In \cref{chap:dataset} we look at the main datasets used, STAR and NExT-QA.
	\item \Cref{chap:method} details experiments that show how current
		models perform on temporal reasoning tasks, and describes our approach
		to generating additional hard negatives focussing on temporal words for
		contrastive training.
	\item \cref{chap:results} shows performance of our model on STAR and NExT-QA.
	\item \cref{chap:discussion} discusses the use of contrastive
		pretraining methods in video and language models, and how our method
		affects performance of temporal reasoning systems.
	\item Finally, the \nameref{chap:conclusion} summarises our findings.
\end{itemize}
