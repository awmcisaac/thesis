%! TeX root = ../charles/en/thesis.tex
%\chapter{Method}
%\label{chap:method}

\chapter{Probing Temporal Ability}
\label{chap:probe}

%\XXX{With the current limited writeup, Ondrej cannot comment yet. Some critical remarks: always try hard to complement other people's scores (which you can cite from a paper) with \emph{your own measurement} of their outputs. Typically, these reported and re-measured numbers will differ and it is always important to know them for a reliable comparison with your own scores.}

In this chapter we probe the existing temporal reasoning abilities of two
models: Merlot Reserve~\citep{zellers2022mreserve} and
VideoCLIP~\citep{xu2021videoclip}.  These models are chosen because of their
use of a contrastive pre-training objective and zero-shot ability to test on
downstream datasets. We describe the required adaptation of STAR to allow for
zero-shot testing of both models before detailing results of targeted data
perturbation for temporal features. The perturbations aim to identify how a
model performs with incorrect data. We would expect a model to be more
uncertain and for accuracy to go down.

\section{Zero-Shot Setup}
\label{sec:mres_zs}

As in~\citet{zellers2022mreserve}, we convert questions into statements to more
closely match the text seen by the model during pre-training. Statements are
rewordings of questions with answers masked out, since YouTube captions do not 
typically render question marks. Since not all of the conversion templates used
for the Merlot Reserve model are given, we develop our own based on examples. 
We use STAR's question-answer template program and modify each question type
into a statement type. For example, for the question and answer pair

\texttt{Q: ``What happened after the person put down the towel?''}

\texttt{A: ``Threw the clothes.''}\\
we are given the (slightly modified) question-answer template from STAR

\texttt{Q: ``What happened after the person [VBP] [NP]?''} 

\texttt{A: ``[Answer]''}.\\
The corresponding statement is

\texttt{S: ``The person \_ after they put down the towel.''}

\texttt{A: ``threw the clothes''},\\
or in templated form

\texttt{S: ``The person \_ after they [VBP] [NP].''}

\texttt{A: ``[answer]''}.\\
where the task becomes correctly identifying the correct answer to replace the
masked span denoted by ``\_''. A full conversion from question type to
statement is given in~\cref{tab:qs_to_stmts}.

Given these statements we present the model with the masked statement for each
instance. This is aligned with the first frame, and all other frames have no
aligned text. The correct answer is the choice most likely to replace the
masked token(s).

%\section{VideoCLIP Zero-Shot Setup}
%\label{sec:videoclip_zs}

For VideoCLIP, we use a similar approach to the zero-shot video question
answering approach in~\citet{xu2021videoclip}. That is, we formulate the task
as a video-text retrieval task, except the candidate answers are associated
with each video and the answer selected is the one that is most relevant out of
the four options. We use the same statement templates as above, with the masked
section replaced by each of four multiple choice options, to again reduce the
domain shift from the pre-training data. The model was pre-trained on
HowTo100M~\citep{miech2019howto100m}, a large-scale dataset of instructional
videos. We found that using these statements produced better results than
keeping the question-answer format. Each candidate answer is ranked according
to the InfoNCE contrastive loss~\citep{oord2019infonce}.

\newgeometry{bottom=4cm}
\begin{landscape}
\begin{table}[t]
    \centering
    \caption{Templates for conversion of all question types to statement types
        in STAR. An ``\_'' in a statement indicates the masked answer.  POS and
        syntax tags are as described in the Penn
        Treebank~\citep{marcus1993penntreebank}. Verbs in Sequence\_T1 and
        Sequence\_T2 are post-processed to ensure grammaticality.}
    \label{tab:qs_to_stmts}
    \scriptsize
    \begin{tabular}{lcc}
        \toprule
        Type & Question & Statement \\
        \midrule
        Interaction\_T1 & Which object was [VBD] by the person? & The object [VBD] by the person was \_. \\
        Interaction\_T2 & What did the person do with [NP]? & The person \_ [NP]. \\
        Interaction\_T3 & What did the person do while they were [VBG] the [NP]? & The person \_ while they were [VBG] the [NP]. \\
        Interaction\_T4 & What did the person do while they were [VBG1] [NP1] and [VBG2] [NP2]? & The person \_ while they were [VBG1] [NP1] and [VBG2] [NP2]. \\
        \midrule
        Sequence\_T1 & Which object did the person [VB1] after they [VBP2] [NP]? & The person [VBP1] \_ after they [VBP2] [NP]. \\
        Sequence\_T2 & Which object did the person [VB1] before they [VBP2] [NP]? & The person [VBP1] \_ before they [VBP2] [NP]. \\
        Sequence\_T3 & What happened after the person [VBP] [NP]? & The person \_ after they [VBP] [NP]. \\
        Sequence\_T4 & What happened before the person [VBP] [NP]? & The person \_ before they [VBP] [NP]. \\
        Sequence\_T5 & What did the person do to [NP2] after [VBP1] [NP1]? & The person \_ [NP2] after [VBP1] [NP1]. \\
        Sequence\_T6 & What did the person do to [NP2] before [VBP1] [NP1]? & The person \_ [NP2] before [VBP1] [NP1]. \\
        \midrule
        Prediction\_T1 & What will the person do next? & The person will \_ next. \\
        Prediction\_T2 & What will the person do next with [NP]? & The person will \_ [NP] next. \\
        Prediction\_T3 & Which object would the person [VB] next? & The person would [VB] \_ next. \\
        Prediction\_T4 & Which object would the person [VB2] next after they [VB1] [NP]? & The person would [VB2] \_ next after they [VB1] [NP]. \\
        \midrule
        Feasibility\_T1 & Which other object is possible to be [VBD] by the person? & The other object possible to be [VBD] by the person is \_. \\
        Feasibility\_T2 & What else is the person able to do with [NP]? & The person is also able to \_ with [NP]. \\
        Feasibility\_T3 & Which object is possible to be [VBP] when the person is [PREP] [NP]? & The object possible to be [VBP] when the person is [PREP] [NP] is \_. \\
        Feasibility\_T4 & What is the person able to do when they are [PREP] [NP]? & The person is able to \_ when they are [PREP] [NP]. \\
        Feasibility\_T5 & Which object is the person able to [VB1] after [VBG2] [NP]? & The person is able to [VB1] \_ after [VBG2] NP. \\
        Feasibility\_T6 & What is the person able to do after [VBG] [NP]? & The person is able to \_ after [VBG] [NP]. \\
        \bottomrule
    \end{tabular}
\end{table}
\end{landscape}
\restoregeometry
\normalsize


\section{Zero-Shot Results}
\label{sec:zs_star}

\Cref{tab:zs_star} shows the reported results on STAR for both models. Our
results on Merlot Reserve differ from the authors' reported results slightly;
this may be down to different conversions from questions to statements. The
models are generally comparable to each other, except VideoCLIP appears to be
better at feasibility questions (e.g. ``Which object is possible to be
taken when the person is in front of the table?'') than Merlot Reserve.

\begin{table}[tp] 
    \centering 
	\caption{Results of models (accuracy) on STAR dataset. I, S, P, F stands
	for Interaction, Sequence, Prediction, and Feasibility. We focus on
	sequence questions, which generally require the most temporal reasoning.}
	% For val, 43.01 mean is weighted by num in each group
    \label{tab:zs_star} 
    \begin{tabular}{lccccc} 
        \toprule
        \multicolumn{1}{c}{}        & \multicolumn{4}{c}{Question Types}                & \multicolumn{1}{c}{} \\
                                      \cmidrule(){2-5}
                                    & I           & S        & P          & F           & Mean \\
        \cmidrule(r){1-1}             \cmidrule(){2-5}                                    \cmidrule(l){6-6}
		Chance						& \multicolumn{5}{c}{25.00} \\
        \cmidrule(r){1-1}             \cmidrule(){2-6}
        %Merlot Reserve (val)        & 43.12       & 42.33    & 43.27      & 47.14       & 43.01 (43.97) \\
		Merlot Reserve (val)        & 43.12       & 42.33    & 43.27      & 47.14     & \textbf{43.97} \\
		VideoCLIP (val)             & 39.66       & \textbf{42.86}    & 48.72    & 50.82       & 42.84 \\
        \cmidrule(r){1-1}             \cmidrule(){2-6}
		Merlot Reserve (test)       & 40.51       & \textbf{44.76}    & 43.85    & 39.48       & 42.15 \\  
		VideoCLIP (test)            & 39.77       & 43.60    & 42.60      & 47.13     & \textbf{43.27} \\
        \cmidrule(r){1-1}             \cmidrule(){2-6}
        Merlot Reserve Paper (test) & 44.8        & 42.4     & 38.8       & 36.2        & 40.5 \\
        \bottomrule
    \end{tabular} 
\end{table} 

Having established the baseline results, we go on to test how we can explore
the temporal reasoning ability through a series of data perturbations.


\section{Changing Temporal Indicators}
\label{sec:change_temp}

The first perturbation explores how the models perform when temporal
expressions are changed, so that a question asks for events occurring at
different times in the video to what the question actually asks, while keeping
the `correct' answer the same. For models that are able to reason across time,
we hypothesise that this should result in worse performance because of the
uncertainty caused by the incorrect time ordering in the question. A model that
does not have temporal reasoning ability would have its capabilities limited
lightly to not at all by this perturbation.

We test on the sequence subset of questions, since every sequence question has
a temporal expression~(before/after) in it, and reverse temporal indicators for
each question. For example, the statement ``The person opened \_ \emph{after}
they took the sandwich'' becomes ``The person opened \_ \emph{before} they took
the sandwich'', with the masked answer remaining \emph{the same} for both
statements.

We report results in~\cref{tab:swap_star}. For both Merlot Reserve and
VideoCLIP models, there is very little difference between either test. In fact,
for Merlot Reserve, the accuracy on this subset actually increases slightly
from the correct statements. This suggests that other factors than temporal
reasoning are contributing to the model's performance in this task.

One reason may be the selection of the other multiple choice options. As
described in~\cref{sec:star}, options are generated to provide distractors
based on compositionality, randomness, and frequency, but not for temporality.
So a model may still pick the most likely option based on its ability to
identify the answer using object detection, where the other three options are
not present in the video at all. To counteract this possibility, we set up a
binary-choice experiment, whereby we change the masked answers of statements to
mask the temporal indicator before or after, creating a binary choice, and
replace the previously masked answer with the correct answer from the dataset.
Since we do not have gold labels for the test data, results are only reported
for validation data. As seen in the `Masked Temporal Indicators' column
in~\cref{tab:swap_star}, both models perform only marginally better than chance
in this instance, suggesting that there is no sense of before or after in
either model.

\begin{table}[tp]
    \centering
	\caption{Statement perturbation for Sequence question types on STAR.
	Swapped indicates that a question has had its temporal expression
	(before/after) reversed. Mask Temporal Expressions asks the model to
	predict, given the two actions, whether one happened before or after
	the other, by masking out the temporal expression. Since the test set
	is withheld, we report results for this only on the validation set.}
    \label{tab:swap_star}
    \begin{tabular}{lccc}
        \toprule
        \multicolumn{1}{c}{}     & \multicolumn{3}{c}{Sequence Questions} \\
                                   \cmidrule(){2-4}
        Model          & Correct & Swapped & Mask Temporal Expressions\\
        \cmidrule(r){1-1} \cmidrule(){2-3} \cmidrule(l){4-4}
		Chance				  & \multicolumn{2}{c}{25.00} & 50.00 \\
        \cmidrule(r){1-1} \cmidrule(){2-3} \cmidrule(l){4-4}
        Merlot Reserve (val)  & 42.33   & 42.92 (+0.59)  & 50.86 \\
        VideoCLIP (val)       & 42.86   & 41.91 (-0.95)  & 50.11 \\
        \cmidrule(r){1-1} \cmidrule(){2-3} \cmidrule(l){4-4}
        Merlot Reserve (test) & 44.76   & 41.04 (-3.72)  & {---} \\
        VideoCLIP (test)      & 43.60   & 43.54 (-0.06)  &  {---} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Randomise Video Frames}

In addition to probing language understanding of temporal expressions, we test
how well the two models encode visual features across frames. Following
\citet{sevilla-lara2021temporal}, who found that performance on action
classification degraded in humans when presented with out-of-order video
frames, we randomise the order of frames presented to the models, and test on
the default created statements. Concretely, for Merlot Reserve this involves
shuffling the order of the 8 frames presented to the model, and aligning the
statement to the new first frame. For VideoCLIP we take the 512-d feature
vectors computed from every 30 frames (see~\cref{sec:videoclip}), and shuffle
based on the time axis. The mean video time is 29.8 seconds, so this involves
shuffling on average 30 feature vectors for each clip. We also experiment with
extracting features from videos that have had all their frames randomly
shuffled. Since Merlot Reserve samples at the frame level anyway, there is no
difference in the two approaches for Merlot Reserve, whereas the computation of
features is affected by a video that has been randomly shuffled for VideoCLIP.
Results are shown in~\cref{tab:shuffle_feats}.

\begin{table}[tp]
    \centering
	\caption{Probing video perturbations. Shuffled video features accuracy on
	STAR test set. VideoCLIP results include both shuffling frame-level features,
	and computing features on a shuffled video. Probing video features results
	in a slight decrease in performance, with a large decrease for shuffled video.}
    \label{tab:shuffle_feats}
    \begin{tabular}{lccccc}
        \toprule
        \multicolumn{1}{c}{}      & \multicolumn{4}{c}{Question Types}  & \multicolumn{1}{c}{} \\
        \cmidrule(){2-5}
        Model                     & I & S & P & F & Mean \\
        \cmidrule(r){1-1} \cmidrule(){2-5} \cmidrule(l){6-6}
        %Merlot Reserve            & 42.08       & 42.58    & 47.28      & 49.2        & 43.28 (45.29) \\
        %Original Merlot Reserve   & 43.12       & 42.33    & 43.27      & 47.14       & 43.01 (43.97) \\
        %\cmidrule(r){1-1} \cmidrule(){2-5} \cmidrule(l){6-6}
        %VideoCLIP (shuffle video) & 34.61       & 36.31    & 36.70      & 40.82       & 36.08 \\
        %VideoCLIP (shuffle frame) & 39.99       & 43.06    & 46.47      & 49.59       & 43.39 \\
        %Original VideoCLIP        & 39.66       & 42.86    & 48.72      & 50.82       & 42.84 \\
		%\midrule
%		On test set & & & & &\\
        Original Merlot Reserve   & 40.51       & 44.76    & 43.85      & 39.48       & 42.15 \\  
		Merlot Reserve			  & 38.88		& 40.61	   & 42.32		& 39.48		  & 40.32 \\
        \cmidrule(r){1-1} \cmidrule(){2-5} \cmidrule(l){6-6}
        Original VideoCLIP        & 39.77       & 43.60    & 42.60      & 47.13       & 43.27 \\
		VideoCLIP (shuffle frame) & 39.27		& 43.31	   & 42.04		& 46.09		  & 42.68 \\
		VideoCLIP (shuffle video) & 33.67       & 35.91    & 36.17      & 34.96       & 35.18 \\
        \bottomrule
    \end{tabular}
\end{table}

We would expect that models with shuffled video features perform worse than the
correct video features. With the exception of the shuffled video for VideoCLIP,
there is little difference in performance between the setups. Computing
features from randomly shuffled videos results in significantly worse
performance due to the weaker features that can be learned from a jumbled-up
video, where features are extracted in the temporal dimension. Shuffling
well-made features, or images in the case of Merlot Reserve, at the frame level
is reasonably robust to these effects, although there is a small drop in
performance.

\section{Summary}
\label{sec:probe_summary}

In this chapter we have looked at the existing capabilities for temporal
reasoning of two state of the art \acrlongpl{vidlm}, Merlot Reserve and
VideoCLIP. We found that their competitive performance on a \acrshort{vidqa}
dataset with questions that should require temporal understanding was not
because of the learned abilities of the models through a targeted probing
setup. By perturbations of the evaluation data, for both language and video, we
have shown that the models are not sensitive to misdirecting inputs, and are
unable to identify (with random chance) the correct ordering of two actions
when asked to determine whether one action occurs before or after another. In
the next chapter we propose a method for learning fine-grained temporal
understanding in the Merlot Reserve model by additional training on targeted
data.


% \section{VideoCLIP STAR results}
% \label{sec:videoclip}
% 
% Zero-shot, statements as in Merlot Reserve
% 
% Validation set: 42.84 \\
% Validation sequence only: 42.86 \\
% Validation sequence only swap before/after in statement: 41.91 \\
% Validation with shuffled frames (all): 36.08 \\
% Validation with shuffled frames (seq only): 36.31 \\
% 
% Use before/after as masked answers: 50.11
% 
% Shuffled frames are from features extracted every 30 frames, so
% generally more frames are chosen than Merlot Reserve (8), as
% average video time is 29.8 seconds.
% Bigger drop-off in performance, but still much better than random
% chance.
