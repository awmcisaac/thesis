%! TeX root = ../charles/en/thesis.tex
%\chapter{Method}
%\label{chap:method}

\chapter{Probing Temporal Ability}
\label{chap:probe}

%\XXX{With the current limited writeup, Ondrej cannot comment yet. Some critical remarks: always try hard to complement other people's scores (which you can cite from a paper) with \emph{your own measurement} of their outputs. Typically, these reported and re-measured numbers will differ and it is always important to know them for a reliable comparison with your own scores.}

In this chapter we probe the existing temporal reasoning abilities of two
models: Merlot Reserve~\citep{zellers2022mreserve} and
VideoCLIP~\citep{xu2021videoclip}.  These models are chosen because of their
use of a contrastive pretraining objective and zero-shot ability to test on
downstream datasets. We describe the required adaptation of STAR to allow for
zero-shot testing of both models before detailing results of targeted data
perturbation for temporal features.

\section{Merlot Reserve Zero-Shot Setup}
\label{sec:mres_zs}

As in~\citet{zellers2022mreserve}, we convert questions into statements to more
closely match the text seen by the model during pre-training. Statements are
rewordings of questions with answers masked out, since YouTube captions do not 
typically render question marks. Since not all of the conversion templates used
for the Merlot Reserve model are given, we develop our own based on examples. 
We use STAR's question-answer template program and modify each question type
into a statement type. For example, for the question and answer pair

Q: ``What happened after the person put down the towel?''

A: ``Threw the clothes.''\\
we are given the (slightly modified) question-answer template from STAR

Q: ``What happened after the person [VBP] [NP]?'' 

A: ``[Answer]''.\\
The corresponding statement is

S: ``The person \_ after they put down the towel.''

A: ``threw the clothes'',\\
or in templated form

S: ``The person \_ after they [VBP] [NP].''

A: ``[answer]''.\\
where the task becomes correctly identifying the correct answer to replace the
masked span denoted by ``\_''. A full conversion from question type to
statement is given in~\cref{tab:qs_to_stmts}.

Given these statements we present the model with four statements for each
instance, where each statement has the masked section replaced by a multiple
choice answer. This is aligned with the first frame, and all other frames have
no aligned text. The correct answer is the choice which gives the most probable
statement, as measured by maximum likelihood.

\begin{landscape}
\begin{table}[htpb]
    \centering
    \caption{Templates for conversion of all question types to statement types
        in STAR. A ``\_'' in a statement indicates the masked answer.  POS and
        syntax tags are as described in the Penn
        Treebank~\cite{marcus1993penntreebank}. Verbs in Sequence\_T1 and
        Sequence\_T2 are post-processed to ensure grammaticality.}
    \label{tab:qs_to_stmts}
    \scriptsize
    \begin{tabular}{lcc}
        \toprule
        Type & Question & Statement \\
        \midrule
        Interaction\_T1 & Which object was [VBD] by the person? & The object [VBD] by the person was \_. \\
        Interaction\_T2 & What did the person do with [NP]? & The person \_ [NP]. \\
        Interaction\_T3 & What did the person do while they were [VBG] the [NP]? & The person \_ while they were [VBG] the [NP]. \\
        Interaction\_T4 & What did the person do while they were [VBG1] [NP1] and [VBG2] [NP2]? & The person \_ while they were [VBG1] [NP1] and [VBG2] [NP2]. \\
        \midrule
        Sequence\_T1 & Which object did the person [VB1] after they [VBP2] [NP]? & The person [VBP1] \_ after they [VBP2] [NP]. \\
        Sequence\_T2 & Which object did the person [VB1] before they [VBP2] [NP]? & The person [VBP1] \_ before they [VBP2] [NP]. \\
        Sequence\_T3 & What happened after the person [VBP] [NP]? & The person \_ after they [VBP] [NP]. \\
        Sequence\_T4 & What happened before the person [VBP] [NP]? & The person \_ before they [VBP] [NP]. \\
        Sequence\_T5 & What did the person do to [NP2] after [VBP1] [NP1]? & The person \_ [NP2] after [VBP1] [NP1]. \\
        Sequence\_T6 & What did the person do to [NP2] before [VBP1] [NP1]? & The person \_ [NP2] before [VBP1] [NP1]. \\
        \midrule
        Prediction\_T1 & What will the person do next? & The person will \_ next. \\
        Prediction\_T2 & What will the person do next with [NP]? & The person will \_ [NP] next. \\
        Prediction\_T3 & Which object would the person [VB] next? & The person would [VB] \_ next. \\
        Prediction\_T4 & Which object would the person [VB2] next after they [VB1] [NP]? & The person would [VB2] \_ next after they [VB1] [NP]. \\
        \midrule
        Feasibility\_T1 & Which other object is possible to be [VBD] by the person? & The other object possible to be [VBD] by the person is \_. \\
        Feasibility\_T2 & What else is the person able to do with [NP]? & The person is also able to \_ with [NP]. \\
        Feasibility\_T3 & Which object is possible to be [VBP] when the person is [PREP] [NP]? & The object possible to be [VBP] when the person is [PREP] [NP] is \_. \\
        Feasibility\_T4 & What is the person able to do when they are [PREP] [NP]? & The person is able to \_ when they are [PREP] [NP]. \\
        Feasibility\_T5 & Which object is the person able to [VB1] after [VBG2] [NP]? & The person is able to [VB1] \_ after [VBG2] NP. \\
        Feasibility\_T6 & What is the person able to do after [VBG] [NP]? & The person is able to \_ after [VBG] [NP]. \\
        \bottomrule
    \end{tabular}
\end{table}
\end{landscape}
\normalsize


\section{VideoCLIP Zero-Shot Setup}
\label{sec:videoclip_zs}

For VideoCLIP, we use a similar approach to the zero-shot video question
answering approach in~\citet{xu2021videoclip}. That is, we formulate the task
as a video-text retrieval task, except the candidate answers are associated
with each video and the answer selected is the one that is most relevant out of
the four options. We use the same statement templates as above, to again reduce
the domain shift from the pre-training data. The model was pre-trained on
HowTo100M~\citep{miech2019howto100m}, a large-scale dataset of instructional
videos. We found that using these masked statements produced better results
than keeping the question-answer format.

\section{Zero-Shot Results}
\label{sec:zs_star}

\Cref{tab:zs_star} shows the reported results on STAR for both models.

\begin{table}[tp] 
    \centering 
    \caption{Results of Merlot Reserve on STAR dataset. Differences explained
        by different sampling techniques I think, or question-statement
        conversions. Also random seed may be different. For val, 43.01 mean is
        weighted by num in each group. I, S, P, F stand for Interaction,
        Sequence, Prediction, Feasibility respectively.}
    \label{tab:zs_star} 
    \begin{tabular}{lccccc} 
        \toprule
        \multicolumn{1}{c}{}        & \multicolumn{4}{c}{Question Types}                & \multicolumn{1}{c}{} \\
                                      \cmidrule(){2-5}
                                    & I & S & P & F & Mean \\
        \cmidrule(r){1-1}             \cmidrule(){2-5}                                    \cmidrule(l){6-6}
        Merlot Reserve (val)        & 43.12       & 42.33    & 43.27      & 47.14       & 43.01 (43.97) \\
        VideoCLIP (val)             & 39.66       & 42.86    & 48.72      & 50.82       & 42.84 \\
        Merlot Reserve (test)       & 40.51       & 44.76    & 43.85      & 39.48       & 42.15 \\  
        VideoCLIP (test)            &        &     &       &        &  \\
        \cmidrule(r){1-1}             \cmidrule(){2-5}                                    \cmidrule(l){6-6}
        Merlot Reserve Paper (test) & 44.8        & 42.4     & 38.8       & 36.2        & 40.5 \\
        \bottomrule
    \end{tabular} 
\end{table} 


\section{Changing Temporal Indicators}
\label{sec:change_temp}

The first perturbation explores how the models perform when temporal
expressions are changed, so that a question asks for events occurring at
different times in the video to what the question actually asks, while keeping
the `correct' answer the same. For models that are able to reason across time,
we hypothesise that this should result in worse performance because of the
uncertainty caused by the incorrect time ordering in the question. A model that
does not have temporal reasoning ability would have its capabilities limited
lightly to not at all by this perturbation.

We test on the sequence subset of questions, since every sequence question has
a temporal expression~(before/after) in it, and reverse temporal indicators for
each question. For example, the statement ``The person opened \_ \emph{after}
they took the sandwich'' becomes ``The person opened \_ \emph{before} they took
the sandwich'', with the masked answer remaining \emph{the same} for both
statements.

We report results in~\cref{tab:swap_star}. For both Merlot Reserve and
VideoCLIP models, there is very little difference between either test. In fact,
for Merlot Reserve, the accuracy on this subset actually increases slightly
from the correct statements. This suggests that other factors than temporal
reasoning are contributing to the model's performance in this task.

One reason may be the selection of the other multiple choice options. As
described in~\cref{sec:star}, options are generated to provide distractors
based on compositionality, randomness, and frequency, but not for temporality.
So a model may still pick the most likely option based on its ability to
identify the answer using object detection, where the other three options are
not present in the video at all. To counteract this possibility, we set up a
binary-choice experiment, whereby we change the masked answers of statements to
mask the temporal indicator before or after, creating a binary choice, and
replace the previously masked answer with the correct answer from the dataset.
Since we do not have gold labels for the test data, results are only reported
for validation data. As seen in the `Masked Temporal Indicators' column
in~\cref{tab:swap_star}, both models perform only marginally better than chance
in this instance, suggesting that there is no sense of before or after in
either model.

\begin{table}[tp]
    \centering
    \caption{Statement perturbation for Sequence question types}
    \label{tab:swap_star}
    \begin{tabular}{lccc}
        \toprule
        \multicolumn{1}{c}{}     & \multicolumn{3}{c}{Sequence Questions} \\
                                   \cmidrule(){2-4}
        Model          & Correct & Swapped & Masked Temporal Expressions\\
        \cmidrule(r){1-1} \cmidrule(){2-3} \cmidrule(l){4-4}
        Merlot Reserve (val)  & 42.33   & 42.92   & 50.86 \\
        Merlot Reserve (test) & 44.76   & TODO   & {---} \\
        VideoCLIP (val)       & 42.84   & 41.91   & 50.11 \\
        VideoCLIP (test)      & TODO   & TODO   &  {---} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Randomise Video Frames}

In addition to probing language understanding of temporal expressions, we test
how well the two models encode visual features across frames. Following
\citet{sevilla-lara2021temporal}, who found that performance on action
classification degraded in humans when presented with out-of-order video
frames, we randomise the order of frames %(TODO:or features, should clarify
more) presented to the models, and test on the default created statements.
Concretely, for Merlot Reserve this involves shuffling the order of the 8
frames presented to the model, and aligning the statement to the new first
frame. For VideoCLIP we take the 512-d feature vectors computed from every 30
frames, and shuffle based on the time axis. The mean video time is 29.8
seconds, so this involves shuffling on average 30 feature vectors for each
clip. We also experiment with extracting features from videos that have had all
their frames randomly shuffled. Results are shown in~\cref{tab:shuffle_feats}.

\begin{table}[tp]
    \centering
    \caption{Shuffle video features (randomly) results on validation set. %(TODO: test set)
		VideoCLIP either compute feats from video with fully shuffled frames,
		or shuffle computed feats on time axis (1 per 30 frames)}
    \label{tab:shuffle_feats}
    \begin{tabular}{lccccc}
        \toprule
        \multicolumn{1}{c}{}      & \multicolumn{4}{c}{Question Types}  & \multicolumn{1}{c}{} \\
        \cmidrule(){2-5}
        Model                     & I & S & P & F & Mean \\
        \cmidrule(r){1-1} \cmidrule(){2-5} \cmidrule(l){6-6}
        Merlot Reserve            & 42.08       & 42.58    & 47.28      & 49.2        & 43.28 (45.29) \\
        Original Merlot Reserve   & 43.12       & 42.33    & 43.27      & 47.14       & 43.01 (43.97) \\
        \cmidrule(r){1-1} \cmidrule(){2-5} \cmidrule(l){6-6}
        VideoCLIP (shuffle frame) & 34.61       & 36.31    & 36.70      & 40.82       & 36.08 \\
        VideoCLIP (shuffle feats) & 39.99       & 43.06    & 46.47      & 49.59       & 43.39 \\
        Original VideoCLIP        & 39.66       & 42.86    & 48.72      & 50.82       & 42.84 \\
        \bottomrule
    \end{tabular}
\end{table}

We would expect that models with shuffled video features perform worse than the
correct video features. With the exception of the shuffled frames for
VideoCLIP, there is little difference in performance between the setups. Computing
features from randomly shuffled videos results in significantly worse performance,
as would be expected from the other cases as well. %TODO: tidy up wording

% \section{VideoCLIP STAR results}
% \label{sec:videoclip}
% 
% Zero-shot, statements as in Merlot Reserve
% 
% Validation set: 42.84 \\
% Validation sequence only: 42.86 \\
% Validation sequence only swap before/after in statement: 41.91 \\
% Validation with shuffled frames (all): 36.08 \\
% Validation with shuffled frames (seq only): 36.31 \\
% 
% Use before/after as masked answers: 50.11
% 
% Shuffled frames are from features extracted every 30 frames, so
% generally more frames are chosen than Merlot Reserve (8), as
% average video time is 29.8 seconds.
% Bigger drop-off in performance, but still much better than random
% chance.
