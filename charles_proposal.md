Title: Temporal Reasoning in Language and Vision Models

Abstract: This thesis will evaluate the performance of language and vision
models on the task of video question answering, with a particular focus on the
ability to reason across time. It will explore the effectiveness of different
methods for combining the different modalities of language and vision, from
joint training of image and language token embeddings [1] to fusion of
modalities via cross-attention mechanisms [2], to techniques for combining
pretrained language models and language and vision models without extra
training [3,4]. The main evaluation dataset will be STAR [5], a benchmark for
situated reasoning in real-world videos. 

References:
[1] Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. (2019a).
VideoBERT: A joint model for video and language representation learning. In
ICCV.  
[2] Zellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A.,
Hessel, J., Farhadi, A., and Choi, Y. (2022). Merlot reserve: Neural script
knowledge through vision and language and sound. In CVPR. 
[3] Zeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A.,
Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V., et al. (2022a). Socratic
models: Composing zero-shot multimodal reasoning with language. arXiv preprint
arXiv:2204.00598.
[4] Su, Y., Lan, T., Liu, Y., Liu, F., Yogatama, D., Wang, Y., Kong, L., and
Collier, N. (2022). Language models can see: Plugging visual controls in text
generation. arXiv preprint arXiv:2205.02655.  
[5] Wu, B., Yu, S., Chen, Z., Tenenbaum, J. B., and Gan, C. (2021). Star: A
benchmark for situated reasoning in real-world videos. In NeurIPS. 
